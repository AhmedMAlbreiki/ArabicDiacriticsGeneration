{"cells":[{"cell_type":"markdown","metadata":{"id":"izh3X-Ds9POp"},"source":["#**Reading the datasets**"]},{"cell_type":"code","source":["#IMPORT YOUR OWN Data\n","dataPath = \"ENTER YOUR DATA PATH\"\n","dictionary_path = 'ENTER YOUR Dictionary PATH'\n","API_KEY = 'ENTER YOUR API KEY'"],"metadata":{"id":"iVhAh3hz_iqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgEHcKto_cn5"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt -O arabic_stopwords.txt\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtgLK_zx1p_k"},"outputs":[],"source":["from google.colab import drive\n","# Mount Google Drive (follow the link and enter the authorization code)\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn4TmR3IOWLJ"},"outputs":[],"source":["!pip install pyspark\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1hq6vJ5FB0W"},"outputs":[],"source":["import os\n","# prompt: read test.txt in pyspark after installing it\n","# Set PYTHONHASHSEED environment variable to '0' before importing PySpark\n","os.environ['PYTHONHASHSEED'] = '0'\n","\n","# Now you can import PySpark and continue with your application\n","from pyspark import SparkContext\n","# Your PySpark application code here\n","\n","import pyspark\n","sc = pyspark.SparkContext()\n","test_data = sc.textFile(dataPath)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeR0Mlu--tKF"},"outputs":[],"source":["!pip install pyspark findspark\n"]},{"cell_type":"markdown","metadata":{"id":"lcxGOOLH3zOJ"},"source":["# **Creating a dataset**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FP6VeR5u374o"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"excel_to_rdd\").getOrCreate()\n","\n","# Read your CSV file into a DataFrame\n","df = spark.read.csv(dictionary_path, header=True, inferSchema=True, encoding=\"UTF-8\")\n","rdd = df.rdd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoQNl6x5AS_v"},"outputs":[],"source":["rdd.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpPqW239Bwzz"},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","import re\n","\n","def strip_arabic_diacritics(word):\n","    if word and isinstance(word, str):\n","        return re.sub(r'[\\u064B-\\u065F]', '', word)\n","    return word\n","\n","strip_arabic_diacritics_udf = udf(strip_arabic_diacritics, StringType())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMGVKeuLBx36"},"outputs":[],"source":["# Assuming your original DataFrame is 'df'\n","df_with_stripped = df.withColumn(\"word_stripped\", strip_arabic_diacritics_udf(df[\"word\"]))\n","\n","\n","# Convert DataFrame to RDD\n","rdd = df_with_stripped.rdd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRh1nY84DI65"},"outputs":[],"source":["def map_function(row):\n","    return (row.word_stripped, row)\n","def reduce_function(value1, value2):\n","    return value1 + [value2] if isinstance(value1, list) else [value1, value2]\n","\n","dictionary = rdd.map(map_function).reduceByKey(reduce_function)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui0N4yWkEU-L"},"outputs":[],"source":["dictionary.collect()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFNVXk53Cwoh"},"outputs":[],"source":["from pyspark.sql.functions import collect_list\n","\n","# Group by the stripped word and collect the original words into a list\n","grouped_df = df_with_stripped.groupBy(\"word_stripped\").agg(collect_list(\"word\").alias(\"original_words\"))\n","\n","# Show the result\n","grouped_df.show(truncate=False)\n"]},{"cell_type":"markdown","metadata":{"id":"J8cplOgkLkmI"},"source":["# **Preprocessing the stemmer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FQrtRMQMqnJ"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt -O arabic_stopwords.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpWSl_tmLoLG"},"outputs":[],"source":["import re\n","\n","# Assuming 'test_data' is your dataset that you want to process\n","\n","with open('arabic_stopwords.txt', 'r', encoding='utf-8') as file:\n","    stopwords = set(file.read().splitlines())\n","\n","# Assuming 'test_data' is a collection of sentences\n","def remove_diacritics(sentence):\n","    words = sentence.split()\n","    return [(re.sub('[ًٌٍَُِّْ]', '', word), idx) for idx, word in enumerate(words)], ['remove_diacritics']\n","\n","def remove_numbers(sentence):\n","    new_sentence, changes = sentence\n","    filtered = [(re.sub(r'\\d+', '', word), idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_numbers']\n","\n","def remove_punctuation(sentence):\n","    new_sentence, changes = sentence\n","    filtered = [(re.sub(r'[^\\w\\s]', '', word), idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_punctuation']\n","\n","def remove_stopwords(sentence, stopwords):\n","    new_sentence, changes = sentence\n","    filtered = [(word, idx) if word not in stopwords else ('', idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_stopwords']\n","\n","def remove_extra_spaces_and_reconstruct(sentence):\n","    new_sentence, changes = sentence\n","    # Filter out the empty tokens and reconstruct the sentence\n","    reconstructed_sentence = ' '.join([word for word, idx in new_sentence if word.strip() != ''])\n","    return reconstructed_sentence, [item for item in new_sentence if item[0].strip() != ''], changes + ['remove_extra_spaces']\n","\n","# Apply transformations\n","test_data_transformed = test_data.map(remove_diacritics) \\\n","                                   .map(remove_punctuation) \\\n","                                   .map(remove_numbers) \\\n","                                   .map(remove_extra_spaces_and_reconstruct)\n","#                                  .map(lambda s: remove_stopwords(s, stopwords)) \\\n","\n","# Collect results to the driver for inspection\n","results = test_data_transformed.collect()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_vyArKRMzFm"},"outputs":[],"source":["# Print results for inspection, limited to 10 iterations\n","for i, (original, (transformed_sentence, word_mappings, changes)) in enumerate(zip(test_data.collect(), results)):\n","    if i >= 10:  # Stop after 10 iterations\n","        break\n","\n","    print(f\"Original: {original}\")\n","    print(f\"Transformed: {transformed_sentence}\")\n","    # We're no longer printing the tokenized version or the changes\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jY6XO7w-OlWh"},"outputs":[],"source":["transformed_sentences = [transformed_sentence for transformed_sentence, _, _ in results]\n","\n","# Now, `transformed_sentences` contains all the full sentences after processing.\n","# You can pass this list to your stemming tool or further processing steps.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAw9qPNPOugw"},"outputs":[],"source":["transformed_sentences"]},{"cell_type":"markdown","metadata":{"id":"47S0-VNA-8Jx"},"source":["# **Stemmer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6IslukH_B4H"},"outputs":[],"source":["pip install farasapy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlwXLZe1JepD"},"outputs":[],"source":["from farasa.pos import FarasaPOSTagger\n","from farasa.ner import FarasaNamedEntityRecognizer\n","from farasa.diacratizer import FarasaDiacritizer\n","from farasa.segmenter import FarasaSegmenter\n","from farasa.stemmer import FarasaStemmer\n","\n","stemmer = FarasaStemmer()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"299bvd97NrwD"},"outputs":[],"source":["!pip install tqdm"]},{"cell_type":"markdown","metadata":{"id":"1vXA_YF0z6hv"},"source":["# **Diacritics Generation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL81ZLej0DTK"},"outputs":[],"source":["!pip install openai\n","import openai"]},{"cell_type":"markdown","metadata":{"id":"G0eGoQ9M8nRU"},"source":["## **one item test**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAp0vvcx6so5"},"outputs":[],"source":["index= 0\n","#test_data.take(50)[index],\n","#test = remove_extra_spaces_and_reconstruct(remove_diacritics(test_data.take(50)[index]))[0], results[index][0], results[index][1]\n","#test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cosfxhvr9kus"},"outputs":[],"source":["from farasa.stemmer import FarasaStemmer\n","import tqdm\n","\n","# Initialize the Farasa stemmer\n","stemmer = FarasaStemmer()\n","\n","\n","def stem_sentence(sentence, filtered_sentence, wordlist):\n","  # Stem the processed sentence\n","  stemmed_sentence = stemmer.stem(sentence)\n","\n","  # Split the stemmed sentence into words assuming spaces as delimiters\n","  stemmed_words = stemmed_sentence.split()\n","\n","  # Process each word-index tuple to append the corresponding stemmed word\n","  new_tuples = []\n","  for word, word_index in tqdm.tqdm(wordlist):\n","      # Ensure the word index is within the bounds of stemmed_words\n","      if word_index < len(stemmed_words):\n","          stemmed_word = stemmed_words[word_index]\n","          new_tuples.append((word_index, stemmed_word, word))\n","      else:\n","          # In case the word index is out of bounds, append None or handle appropriately\n","          new_tuples.append((word, word_index, None))\n","  return sentence, filtered_sentence, new_tuples\n","\n","# Now, `new_tuples` contains tuples of the form (original word, word index, stemmed word)\n","#test = test_data.take(500)[index], remove_extra_spaces_and_reconstruct(remove_diacritics(test_data.take(50)[index]))[0], results[index][0], new_tuples\n","\n","#test\n","#stem_sentence(remove_extra_spaces_and_reconstruct(remove_diacritics(test_data.take(50)[index]))[0], results[index][0],  results[index][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7NmZuenL_MV"},"outputs":[],"source":["dictionary.lookup(\"أول\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrg0wUJaPTXG"},"outputs":[],"source":["dictionary.take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7Ats58mLu4G"},"outputs":[],"source":["#test[3][15][2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qPSkD5RD900"},"outputs":[],"source":["from openai import OpenAI\n","\n","client = OpenAI(api_key='your_api_key_here')\n","\n","def generate_oneWord_diacritics(word,stemmed_word, sentence, dictionary):\n","    # Contextual explanation of the word within a sentence\n","    prompt_explanation = \"في سياق الجملة '{}', ما هو المعنى الدقيق لكلمة '{}'؟\".format(sentence, word)\n","    #print(prompt_explanation)\n","\n","    # Derivation and meaning matching\n","    dictionary_meaning = dictionary.lookup(stemmed_word)\n","    prompt_meaning = \"كلمة '{}' مشتقة من '{}'. أي من المعاني التالية يتوافق مع استخدام '{}' في السياق المذكور: {}؟\".format(stemmed_word, word, word, dictionary_meaning)\n","\n","    # Diacritic addition based on meaning\n","    prompt_diacritics = \"أضف التشكيلات الصوتية لكلمة '{}' في جملة '{}' بناءً على معنى '{}' كما تم تحديده أعلاه.\".format(word, sentence, stemmed_word)\n","    client = OpenAI(api_key=API_KEY)\n","    # Sending the requests to OpenAI API with structured conversation\n","    response = client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages = [\n","            {\"role\": \"system\", \"content\": \"أنت خبير لغوي، يرجى تقديم توضيح للمعنى وتشكيل الكلمة.\"},\n","            {\"role\": \"user\", \"content\": \"في سياق الجملة '{}', ما المعنى الدقيق لكلمة '{}'؟\".format(sentence, word)},\n","            {\"role\": \"system\", \"content\": \"استنادًا إلى المعنى المفسر أعلاه، قم بتوقع التشكيل الصحيح لجذر كلمة '{}' في جملة '{}'.\".format(stemmed_word, sentence)},\n","            {\"role\": \"user\", \"content\": \"كلمة '{}' مشتقة من '{}'. بناءً على سياق الجملة '{}', قم بتوقع التشكيل لجذر كلمة '{}'.\".format(stemmed_word, word, sentence, word)},\n","            {\"role\": \"system\", \"content\": \"الآن، بناءً على التشكيل المقترح للجذر والسياق المحدد، قم بإضافة التشكيلات اللازمة للكلمة بأكملها.\"},\n","            {\"role\": \"user\", \"content\": \"أضف التشكيلات الصوتية لكلمة '{}' في الجملة '{}' لتوضيح معناها المقصود.\".format(word, sentence)}\n","        ],\n","        temperature=0.2,\n","        top_p=1,\n","        frequency_penalty=0,\n","        presence_penalty=0\n","    )\n","    return response.choices[0].message.content\n","\n","# Example usage (make sure to define 'test' and 'dictionary' appropriately)\n","#x = generate_oneWord_diacritics(test[3][15], test[1], dictionary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzxlDxDMFonQ"},"outputs":[],"source":["import string\n","\n","\n","def replace_word(word,sentence,word_index):\n","    sentence_split = sentence.split()\n","    sentence_split[word_index] = word\n","    return \" \".join(sentence_split)\n","\n","def update_diacritcs(LLM_output, word, sentence, word_index):\n","  LLM_output = LLM_output.translate(str.maketrans('', '', string.punctuation))\n","  diacriticized_words = LLM_output.split()\n","  for diacriticized_word in diacriticized_words:\n","    if (remove_diacritics(diacriticized_word)[0][0][0] == word) & (len(remove_diacritics(diacriticized_word)[0][0][0]) != len(diacriticized_word)):\n","      y = replace_word(diacriticized_word,sentence,word_index)\n","      return y\n","  return sentence\n","\n","def create_diacritics(inputSentence, dictionary):\n","  output = inputSentence[0]\n","  for word_index, stemmed_word, word in inputSentence[2]:\n","    LLM_output = generate_oneWord_diacritics(word,stemmed_word, output, dictionary)\n","    output = update_diacritcs(LLM_output, word, output, word_index)\n","  return output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcS0oWfreliY"},"outputs":[],"source":["#create_diacritics(test[1:], dictionary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgeQfgwtbshW"},"outputs":[],"source":["#test[1:][2][0][2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuP0wTAiM9jK"},"outputs":[],"source":["#test[3][15][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x39tIuLWKn9e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wc0K-DHRIAZw"},"outputs":[],"source":["#remove_punctuation(remove_diacritics(diacriticized_word[4]))[0][0][0] == test[3][15][2]\n"]},{"cell_type":"markdown","metadata":{"id":"pNGMiw1n8zN4"},"source":["## **Bulk test**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTqQbqs_0IYZ"},"outputs":[],"source":["#test = test_data.take(500)[index], remove_extra_spaces_and_reconstruct(remove_diacritics(test_data.take(500)[index]))[0], results[index][0], new_tuples\n","#stem_sentence(remove_extra_spaces_and_reconstruct(remove_diacritics(test_data.take(50)[index]))[0], results[index][0],  results[index][1])\n","\n","\n","def save_checkpoint(data, filename):\n","    \"\"\"Save the data to a file.\"\"\"\n","    with open(filename, 'a') as file:  # 'a' mode to append to the file\n","        for line in data:\n","            file.write(line + '\\n')\n","\n","# Initialize a list to store the outputs\n","outputs = []\n","data = test_data.take(501)\n","for index in range(30,49):\n","    target = data[index]\n","    filtered_text = remove_extra_spaces_and_reconstruct(remove_diacritics(target))[0]\n","    sample = stem_sentence(filtered_text, results[index][0], results[index][1])\n","    generated_text = create_diacritics(sample, dictionary)\n","\n","    # Collect the current iteration's output\n","    outputs.append(f\"Target Text: {target}, Filtered Text: {filtered_text}, Generated Text: {generated_text}\")\n","\n","    # Checkpoint every 10 iterations\n","    if (index + 1) % 10 == 0:\n","        save_checkpoint(outputs, f'/content/drive/MyDrive/BulkTestGPT4NoDictionary/outputs_checkpoint_{index // 10}.txt')\n","        outputs = []  # Reset the outputs list for the next batch\n","\n","# Save any remaining outputs after the final iteration\n","if outputs:\n","    save_checkpoint(outputs, f'/content/drive/MyDrive/BulkTestGPT4NoDictionary/outputs_checkpoint_final.txt')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fb37gmOjDtJH"},"source":["# **Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMnI3h_eD2tj"},"outputs":[],"source":["pip install Levenshtein"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHCutLU2Z3RF"},"outputs":[],"source":["import pandas as pd\n","import os\n","import glob\n","\n","def read_checkpoint_files(folder_path):\n","    # Pattern to match all checkpoint files\n","    file_pattern = os.path.join(folder_path, 'outputs_checkpoint_*.txt')\n","    # List of all matching files\n","    file_list = glob.glob(file_pattern)\n","\n","    # Initialize an empty list to store DataFrame rows before concatenation\n","    rows = []\n","\n","    # Read each file\n","    for file in file_list:\n","        with open(file, 'r') as f:\n","            for line in f:\n","                try:\n","                    # Parse the line\n","                    parts = line.split(', ')\n","                    target_text = parts[0].split('Target Text: ')[1]\n","                    filtered_text = parts[1].split('Filtered Text: ')[1]\n","                    generated_text = parts[2].split('Generated Text: ')[1].strip()\n","\n","                    # Append to list as dictionary\n","                    rows.append({'Target Text': target_text,\n","                                 'Filtered Text': filtered_text,\n","                                 'Generated Text': generated_text})\n","                except IndexError:\n","                    # Handle lines that do not match the expected format\n","                    print(f\"Skipping line due to unexpected format: {line}\")\n","\n","    # Convert the list of dictionaries to a DataFrame\n","    df = pd.DataFrame(rows)\n","    return df\n","\n","# Specify the folder path where your files are saved\n","folder_path = '/content/drive/MyDrive/BulkTestGPT4NoDictionary'\n","df = read_checkpoint_files(folder_path)\n","\n","# Now `df` contains all your data\n","print(df.head())  # Print the first few rows to check\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WflSrM0cNRI"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMGYX5_vZZ7l"},"outputs":[],"source":["def clean_text(text):\n","    # Regular expression to keep Arabic letters, diacritics, and spaces, excluding common Arabic punctuation\n","    pattern = r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\u064B-\\u065F\\u0670\\u08D3-\\u08E1\\s]'\n","    arabic_punctuations = r'[،؛؟٪٫٬٭؉۔]'\n","\n","    # Remove characters not matched by the pattern\n","    cleaned_text = re.sub(pattern, '', text)\n","    # Remove Arabic punctuations\n","    cleaned_text = re.sub(arabic_punctuations, '', cleaned_text)\n","    # Remove extra spaces\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n","\n","    return cleaned_text\n","\n","\n","def align_text(original_text, generated_text):\n","    aligned_original = []\n","    aligned_generated = []\n","    original_text = clean_text(original_text)\n","    generated_text = clean_text(generated_text)\n","    original_index = 0\n","    generated_index = 0\n","\n","    while original_index < len(original_text) or generated_index < len(generated_text):\n","        original_text\n","        original_char = original_text[original_index] if original_index < len(original_text) else ''\n","        generated_char = generated_text[generated_index] if generated_index < len(generated_text) else ''\n","\n","        if (original_char in \"ًٌٍَُِّْ|\" and generated_char in \"ًٌٍَُِّْ|\") or (original_char not in \"ًٌٍَُِّْ|\" and generated_char not in \"ًٌٍَُِّْ|\"):\n","            aligned_original.append(original_char)\n","            aligned_generated.append(generated_char)\n","            original_index += 1\n","            generated_index += 1\n","        elif original_char not in \"ًٌٍَُِّْ|\":\n","            aligned_original.append('|')\n","            aligned_generated.append(generated_char)\n","            generated_index += 1\n","        else:\n","            aligned_original.append(original_char)\n","            aligned_generated.append('|')\n","            original_index += 1\n","\n","    aligned_original_text = ''.join(aligned_original)\n","    aligned_generated_text = ''.join(aligned_generated)\n","\n","    return aligned_original_text, aligned_generated_text\n","\n","\n","# Sample original text and generated text\n","original_text = \"123()الكَتابُ جَيدٌ جداً\"\n","generated_text = \"123)الكِتابُ جيِّدٌ جداً\"\n","\n","# Align the texts\n","aligned_original_text, aligned_generated_text = align_text(original_text, generated_text)\n","\n","# Print the aligned texts\n","print(f\"Original Text: {aligned_original_text}\")\n","print(f\"Generated Text: {aligned_generated_text}\")\n","\n","\n","for index in range(df.shape[0]):\n","    aligned_target_text, aligned_generated_text = align_text(df.loc[index,\"Target Text\"], df.loc[index,\"Generated Text\"])\n","    df.at[index, 'Aligned Target Text'] = aligned_target_text\n","    df.at[index, 'Aligned Generated Text'] = aligned_generated_text\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjCJHgaG6Q5c"},"outputs":[],"source":["import re\n","\n","\n","def clean_text(text):\n","    # Regular expression to keep Arabic letters, diacritics, and spaces, excluding common Arabic punctuation\n","    pattern = r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\u064B-\\u065F\\u0670\\u08D3-\\u08E1\\s]'\n","    arabic_punctuations = r'[،؛؟٪٫٬٭؉۔]'\n","\n","    # Remove characters not matched by the pattern\n","    cleaned_text = re.sub(pattern, '', text)\n","    # Remove Arabic punctuations\n","    cleaned_text = re.sub(arabic_punctuations, '', cleaned_text)\n","    # Remove extra spaces\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n","\n","    return cleaned_text\n","\n","# Sample text that includes Arabic characters, diacritics, and some unwanted characters\n","text = \"مرحبًا بالعالم! 123 هذا اختبار... [هل أنت جاهز؟] {نعم، أنا كذلك!}، وأيضًا؛\"\n","\n","# Clean the text\n","cleaned_text = clean_text(text)\n","\n","print(\"Original Text:\", text)\n","print(\"Cleaned Text:\", cleaned_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BGDL6vh1e1Z"},"outputs":[],"source":["df.loc[4,\"Target Text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8sjxk1yghM-"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doOJtTDAlUHp"},"outputs":[],"source":["def remove_non_diacritics(text):\n","    diacritic_characters = \"ًٌٍَُِّْ |\"\n","    result = [char for char in text if char in diacritic_characters]\n","    return ''.join(result)\n","\n","# Example usage:\n","text = \"الكِتاب جيِّدٌ جداً\"\n","diacritic_text = remove_non_diacritics(text)\n","print(diacritic_text)\n","\n","\n","for index in range(df.shape[0]):\n","    filtered_target_text, filtered_generated_text = remove_non_diacritics(df.loc[index,\"Aligned Target Text\"]),(remove_non_diacritics(df.loc[index,\"Aligned Generated Text\"]))\n","    df.at[index, 'Filtered Target Text'] = filtered_target_text\n","    df.at[index, 'Filtered Generated Text'] = filtered_generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaNinY-BDxhU"},"outputs":[],"source":["def remove_corresponding_indices(target_text, generated_text):\n","    # Initialize empty strings for the new output\n","    new_generated_text = \"\"\n","    new_target_text = \"\"\n","\n","    # Initialize an index for iterating over the characters\n","    i = 0\n","\n","    # Iterate over the characters in the generated text\n","    while i < len(generated_text) and i < len(target_text):\n","        # If either character is '|', increase the index and skip adding the characters\n","        if generated_text[i] == '|' or target_text[i] == '|':\n","            i += 1\n","            continue\n","\n","        # Add characters to new strings if there's no '|' at the current index in either string\n","        new_generated_text += generated_text[i]\n","        new_target_text += target_text[i]\n","        i += 1\n","\n","    return new_target_text,  new_generated_text\n","\n","\n","\n","\n","def word_accuracy(reference, generated):\n","    reference_words = reference.split()\n","    generated_words = generated.split()\n","\n","    correct_words = sum(1 for ref, gen in zip(reference_words, generated_words) if ref == gen)\n","    total_words = len(reference_words)\n","\n","    accuracy = correct_words / total_words\n","    return accuracy\n","def character_accuracy(reference, generated):\n","    correct_chars = sum(1 for ref, gen in zip(reference, generated) if ref == gen)\n","    total_chars = len(reference)\n","\n","    if total_chars == 0:\n","        return 0  # Or return None or any other value that signifies undefined accuracy\n","\n","    accuracy = correct_chars / total_chars\n","    return accuracy\n","import Levenshtein\n","\n","def levenshtein_distance(reference, generated):\n","    distance = Levenshtein.distance(reference, generated)\n","    return distance\n","\n","# Sample reference text and generated text (with diacritics)\n","reference_text = remove_non_diacritics(aligned_original_text)\n","generated_text = remove_non_diacritics(aligned_generated_text)\n","\n","# Calculate metrics\n","word_acc = word_accuracy(reference_text, generated_text)\n","char_acc = character_accuracy(reference_text, generated_text)\n","levenshtein_dist = levenshtein_distance(reference_text, generated_text)\n","new_target, new_generated = remove_corresponding_indices(reference_text, generated_text)\n","diacritics_acc = character_accuracy(new_target, new_generated)\n","\n","# Print results\n","print(f\"Reference Text: {reference_text}\")\n","print(f\"Generated Text: {generated_text}\")\n","print(f\"Word Accuracy: {word_acc * 100:.2f}%\")\n","print(f\"Character Accuracy: {char_acc * 100:.2f}%\")\n","print(f\"Levenshtein Distance: {levenshtein_dist}\")\n","print(f\"Diacritics Accuracy: {diacritics_acc * 100:.2f}%\")\n","\n","\n","for index in range(df.shape[0]):\n","    word_acc = word_accuracy(df.loc[index,\"Filtered Target Text\"], df.loc[index,\"Filtered Generated Text\"])\n","    char_acc = character_accuracy(df.loc[index,\"Filtered Target Text\"], df.loc[index,\"Filtered Generated Text\"])\n","    levenshtein_dist = levenshtein_distance(df.loc[index,\"Filtered Target Text\"], df.loc[index,\"Filtered Generated Text\"])\n","    new_target, new_generated = remove_corresponding_indices(df.loc[index,\"Filtered Target Text\"], df.loc[index,\"Filtered Generated Text\"])\n","    diacritics_acc = character_accuracy(new_target, new_generated)\n","    df.at[index, 'Word Accuracy'] = word_acc\n","    df.at[index, 'Character Accuracy'] = char_acc\n","    df.at[index, 'Levenshtein Distance'] = levenshtein_dist\n","    df.at[index, 'Diacritics Accuracy'] = diacritics_acc\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5OVPHhF8lGl"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming 'df' is your DataFrame\n","\n","# Specify the directory where you want to save the file\n","directory_path = '/content/drive/MyDrive/BulkTestGPT4NoDictionary'\n","\n","# Specify the filename\n","filename = 'GPT4Results.csv'\n","\n","# Full path\n","full_path = f'{directory_path}/{filename}'\n","\n","# Save the DataFrame to CSV\n","df.to_csv(full_path, index=False)\n","\n","print(f'DataFrame saved to {full_path}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFswQVGGarae"},"outputs":[],"source":["df.describe()"]}],"metadata":{"colab":{"collapsed_sections":["47S0-VNA-8Jx"],"machine_shape":"hm","provenance":[{"file_id":"1ESsuNm6FkUDfptRoUS-J153NitN9E1uf","timestamp":1708783841585},{"file_id":"1mcz0D6_CSxP7PI-lZJDGIhYPzbaII6bn","timestamp":1708699533916},{"file_id":"1vciZ-7uIJ-9I8XOV1jMb_i3kkqA6isXR","timestamp":1708592281472},{"file_id":"1ytLiTT5Q7TAbFuWam787_CyCBpaBQD_U","timestamp":1708592253580},{"file_id":"1b1A3eb6dLZisxvp1KiTXOAXULF_kX2Ly","timestamp":1708591444026}],"mount_file_id":"1mcz0D6_CSxP7PI-lZJDGIhYPzbaII6bn","authorship_tag":"ABX9TyNLFc+NtNTpAjPuGWU+F5oi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}