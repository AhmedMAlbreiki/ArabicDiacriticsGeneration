{"cells":[{"cell_type":"markdown","metadata":{"id":"izh3X-Ds9POp"},"source":["#**Reading the datasets**"]},{"cell_type":"code","source":["dataPath = \"ENTER YOUR TRAIN/VALIDATION DATA PATH\"\n","dictionary_path = 'ENTER YOUR Dictionary PATH'"],"metadata":{"id":"DtOpRp4QCGNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgEHcKto_cn5"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt -O arabic_stopwords.txt\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtgLK_zx1p_k"},"outputs":[],"source":["from google.colab import drive\n","# Mount Google Drive (follow the link and enter the authorization code)\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn4TmR3IOWLJ"},"outputs":[],"source":["!pip install pyspark\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1hq6vJ5FB0W"},"outputs":[],"source":["import os\n","# prompt: read train.txt in pyspark after installing it\n","# Set PYTHONHASHSEED environment variable to '0' before importing PySpark\n","os.environ['PYTHONHASHSEED'] = '0'\n","\n","# Now you can import PySpark and continue with your application\n","from pyspark import SparkContext\n","# Your PySpark application code here\n","\n","import pyspark\n","sc = pyspark.SparkContext()\n","train_data = sc.textFile(dataPath)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeR0Mlu--tKF"},"outputs":[],"source":["!pip install pyspark findspark\n"]},{"cell_type":"markdown","metadata":{"id":"lcxGOOLH3zOJ"},"source":["# **Creating a dataset**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FP6VeR5u374o"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"excel_to_rdd\").getOrCreate()\n","\n","# Read your CSV file into a DataFrame\n","df = spark.read.csv(dictionary_path, header=True, inferSchema=True, encoding=\"UTF-8\")\n","rdd = df.rdd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoQNl6x5AS_v"},"outputs":[],"source":["rdd.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpPqW239Bwzz"},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","import re\n","\n","def strip_arabic_diacritics(word):\n","    if word and isinstance(word, str):\n","        return re.sub(r'[\\u064B-\\u065F]', '', word)\n","    return word\n","\n","strip_arabic_diacritics_udf = udf(strip_arabic_diacritics, StringType())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMGVKeuLBx36"},"outputs":[],"source":["# Assuming your original DataFrame is 'df'\n","df_with_stripped = df.withColumn(\"word_stripped\", strip_arabic_diacritics_udf(df[\"word\"]))\n","\n","\n","# Convert DataFrame to RDD\n","rdd = df_with_stripped.rdd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRh1nY84DI65"},"outputs":[],"source":["def map_function(row):\n","    return (row.word_stripped, row)\n","def reduce_function(value1, value2):\n","    return value1 + [value2] if isinstance(value1, list) else [value1, value2]\n","\n","dictionary = rdd.map(map_function).reduceByKey(reduce_function)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui0N4yWkEU-L"},"outputs":[],"source":["dictionary.collect()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFNVXk53Cwoh"},"outputs":[],"source":["from pyspark.sql.functions import collect_list\n","\n","# Group by the stripped word and collect the original words into a list\n","grouped_df = df_with_stripped.groupBy(\"word_stripped\").agg(collect_list(\"word\").alias(\"original_words\"))\n","\n","# Show the result\n","grouped_df.show(truncate=False)\n"]},{"cell_type":"markdown","metadata":{"id":"J8cplOgkLkmI"},"source":["# **Preprocessing the stemmer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FQrtRMQMqnJ"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt -O arabic_stopwords.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpWSl_tmLoLG"},"outputs":[],"source":["import re\n","\n","# Assuming 'train_data' is your dataset that you want to process\n","\n","with open('arabic_stopwords.txt', 'r', encoding='utf-8') as file:\n","    stopwords = set(file.read().splitlines())\n","\n","# Assuming 'train_data' is a collection of sentences\n","def remove_diacritics(sentence):\n","    words = sentence.split()\n","    return [(re.sub('[ًٌٍَُِّْ]', '', word), idx) for idx, word in enumerate(words)], ['remove_diacritics']\n","\n","def remove_numbers(sentence):\n","    new_sentence, changes = sentence\n","    filtered = [(re.sub(r'\\d+', '', word), idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_numbers']\n","\n","def remove_punctuation(sentence):\n","    new_sentence, changes = sentence\n","    filtered = [(re.sub(r'[^\\w\\s]', '', word), idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_punctuation']\n","\n","def remove_stopwords(sentence, stopwords):\n","    new_sentence, changes = sentence\n","    filtered = [(word, idx) if word not in stopwords else ('', idx) for word, idx in new_sentence]\n","    return filtered, changes + ['remove_stopwords']\n","\n","def remove_extra_spaces_and_reconstruct(sentence):\n","    new_sentence, changes = sentence\n","    # Filter out the empty tokens and reconstruct the sentence\n","    reconstructed_sentence = ' '.join([word for word, idx in new_sentence if word.strip() != ''])\n","    return reconstructed_sentence, [item for item in new_sentence if item[0].strip() != ''], changes + ['remove_extra_spaces']\n","\n","# Apply transformations\n","train_data_transformed = train_data.map(remove_diacritics) \\\n","                                   .map(remove_punctuation) \\\n","                                   .map(remove_numbers) \\\n","                                   .map(remove_extra_spaces_and_reconstruct)\n","#                                  .map(lambda s: remove_stopwords(s, stopwords)) \\\n","\n","# Collect results to the driver for inspection\n","results = train_data_transformed.collect()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_vyArKRMzFm"},"outputs":[],"source":["# Print results for inspection, limited to 10 iterations\n","for i, (original, (transformed_sentence, word_mappings, changes)) in enumerate(zip(train_data.collect(), results)):\n","    if i >= 10:  # Stop after 10 iterations\n","        break\n","\n","    print(f\"Original: {original}\")\n","    print(f\"Transformed: {transformed_sentence}\")\n","    # We're no longer printing the tokenized version or the changes\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jY6XO7w-OlWh"},"outputs":[],"source":["transformed_sentences = [transformed_sentence for transformed_sentence, _, _ in results]\n","\n","# Now, `transformed_sentences` contains all the full sentences after processing.\n","# You can pass this list to your stemming tool or further processing steps.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAw9qPNPOugw"},"outputs":[],"source":["transformed_sentences"]},{"cell_type":"markdown","metadata":{"id":"47S0-VNA-8Jx"},"source":["# **Stemmer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6IslukH_B4H"},"outputs":[],"source":["pip install farasapy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlwXLZe1JepD"},"outputs":[],"source":["from farasa.pos import FarasaPOSTagger\n","from farasa.ner import FarasaNamedEntityRecognizer\n","from farasa.diacratizer import FarasaDiacritizer\n","from farasa.segmenter import FarasaSegmenter\n","from farasa.stemmer import FarasaStemmer\n","\n","stemmer = FarasaStemmer()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"299bvd97NrwD"},"outputs":[],"source":["!pip install tqdm"]},{"cell_type":"markdown","metadata":{"id":"1vXA_YF0z6hv"},"source":["# **Diacritics Generation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL81ZLej0DTK"},"outputs":[],"source":["!pip install openai\n","import openai"]},{"cell_type":"markdown","metadata":{"id":"G0eGoQ9M8nRU"},"source":["## **one item test**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAp0vvcx6so5"},"outputs":[],"source":["index= 0\n","#train_data.take(50)[index],\n","test = remove_extra_spaces_and_reconstruct(remove_diacritics(train_data.take(50)[index]))[0], results[index][0], results[index][1]\n","test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cosfxhvr9kus"},"outputs":[],"source":["from farasa.stemmer import FarasaStemmer\n","import tqdm\n","\n","# Initialize the Farasa stemmer\n","stemmer = FarasaStemmer()\n","\n","\n","def stem_sentence(sentence, filtered_sentence, wordlist):\n","  # Stem the processed sentence\n","  stemmed_sentence = sentence\n","\n","  # Split the stemmed sentence into words assuming spaces as delimiters\n","  stemmed_words = stemmed_sentence.split()\n","\n","  # Process each word-index tuple to append the corresponding stemmed word\n","  new_tuples = []\n","  for word, word_index in wordlist:\n","      # Ensure the word index is within the bounds of stemmed_words\n","      if word_index < len(stemmed_words):\n","          stemmed_word = stemmed_words[word_index]\n","          new_tuples.append((word_index, stemmed_word, word))\n","      else:\n","          # In case the word index is out of bounds, append None or handle appropriately\n","          new_tuples.append((word, word_index, None))\n","  return sentence, filtered_sentence, new_tuples\n","\n","# Now, `new_tuples` contains tuples of the form (original word, word index, stemmed word)\n","#test = train_data.take(500)[index], remove_extra_spaces_and_reconstruct(remove_diacritics(train_data.take(50)[index]))[0], results[index][0], new_tuples\n","\n","#test\n","#stem_sentence(remove_extra_spaces_and_reconstruct(remove_diacritics(train_data.take(50)[index]))[0], results[index][0],  results[index][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7NmZuenL_MV"},"outputs":[],"source":["dictionary.lookup(\"أول\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrg0wUJaPTXG"},"outputs":[],"source":["dictionary.take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7Ats58mLu4G"},"outputs":[],"source":["#test[3][15][2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qPSkD5RD900"},"outputs":[],"source":["\n","\n","def generate_oneWord_diacritics(word,stemmed_word, sentence, dictionary,target):\n","    # Contextual explanation of the word within a sentence\n","    prompt_direct_diacritics = \"بالنظر إلى الجملة التالية: '{}', يرجى تحليل استخدام كلمة '{}' وإضافة التشكيلات الصوتية اللازمة لها بناءً على معناها في هذا السياق. تأكد من أن التشكيل يعكس النطق الدقيق للكلمة ويتوافق مع دورها النحوي والمعنوي في الجملة.\".format(sentence, word)\n","\n","    # Sending the request to OpenAI API with a simplified and direct prompt\n","\n","    x = {\"messages\" : [\n","        {\"role\": \"system\", \"content\": \"أنت خبير لغوي، يرجى تقديم توضيح للمعنى وتشكيل الكلمة بشكل مباشر.\"},\n","        {\"role\": \"user\", \"content\": prompt_direct_diacritics},\n","        {\"role\": \"assistant\", \"content\": target}\n","    ]}\n","\n","    return x\n","\n","# Example usage (make sure to define 'test' and 'dictionary' appropriately)\n","#x = generate_oneWord_diacritics(test[3][15], test[1], dictionary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzxlDxDMFonQ"},"outputs":[],"source":["import string\n","\n","\n","def replace_word(word,sentence,word_index):\n","    sentence_split = sentence.split()\n","    sentence_split[word_index] = word\n","    return \" \".join(sentence_split)\n","\n","def update_diacritcs(LLM_output, word, sentence, word_index):\n","  LLM_output = LLM_output.translate(str.maketrans('', '', string.punctuation))\n","  diacriticized_words = LLM_output.split()\n","  for diacriticized_word in diacriticized_words:\n","    if (remove_diacritics(diacriticized_word)[0][0][0] == word) & (len(remove_diacritics(diacriticized_word)[0][0][0]) != len(diacriticized_word)):\n","      y = replace_word(diacriticized_word,sentence,word_index)\n","      return y\n","  return sentence\n","\n","def create_diacritics(inputSentence, dictionary):\n","  output = inputSentence[0]\n","  for word_index, stemmed_word, word in inputSentence[2]:\n","    LLM_output = generate_oneWord_diacritics(word,stemmed_word, output, dictionary)\n","    #output = update_diacritcs(LLM_output, word, output, word_index)\n","  return LLM_output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcS0oWfreliY"},"outputs":[],"source":["#create_diacritics(test[1:], dictionary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgeQfgwtbshW"},"outputs":[],"source":["#test[1:][2][0][2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuP0wTAiM9jK"},"outputs":[],"source":["#test[3][15][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wc0K-DHRIAZw"},"outputs":[],"source":["#remove_punctuation(remove_diacritics(diacriticized_word[4]))[0][0][0] == test[3][15][2]\n"]},{"cell_type":"markdown","metadata":{"id":"pNGMiw1n8zN4"},"source":["## **Bulk test**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTqQbqs_0IYZ"},"outputs":[],"source":["#test = train_data.take(500)[index], remove_extra_spaces_and_reconstruct(remove_diacritics(train_data.take(500)[index]))[0], results[index][0], new_tuples\n","#stem_sentence(remove_extra_spaces_and_reconstruct(remove_diacritics(train_data.take(50)[index]))[0], results[index][0],  results[index][1])\n","\n","\n","def save_checkpoint(data, filename):\n","    \"\"\"Save the data to a file.\"\"\"\n","    with open(filename, 'a') as file:  # 'a' mode to append to the file\n","        for line in data:\n","            file.write(line + '\\n')\n","dataset = []\n","# Initialize a list to store the outputs\n","outputs = []\n","data = train_data.take(501)\n","for index in range(500):\n","    target = data[index]\n","    filtered_text = remove_extra_spaces_and_reconstruct(remove_diacritics(target))[0]\n","    sample = stem_sentence(filtered_text, results[index][0], results[index][1])\n","    output = sample[0]\n","    for word_index, stemmed_word, word in sample[2]:\n","\n","      LLM_output = generate_oneWord_diacritics(word,stemmed_word, output, dictionary,data[index].split()[word_index])\n","      dataset.append(LLM_output)\n","\n","#    generated_text = create_diacritics(sample, dictionary)\n","\n","    # Collect the current iteration's output\n","\n","#    # Checkpoint every 10 iterations\n","#    if (index + 1) % 10 == 0:\n","#        save_checkpoint(outputs, f'/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetunedDataset/Dataset_checkpoint_{index // 10}.txt')\n","#        outputs = []  # Reset the outputs list for the next batch\n","#\n","## Save any remaining outputs after the final iteration\n","#if outputs:\n","#    save_checkpoint(outputs, f'/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetuned/Dataset_checkpoint_final.txt')\n","\n"]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"zOJOliSXLg4p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## error check the data"],"metadata":{"id":"SapARhGiScrA"}},{"cell_type":"code","source":["!pip -q install datasets tiktoken openai"],"metadata":{"id":"BW696ALaSJJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Format error checks\n","import json\n","import os\n","import tiktoken\n","import numpy as np\n","from collections import defaultdict\n","\n","format_errors = defaultdict(int)\n","\n","for ex in dataset:\n","    if not isinstance(ex, dict):\n","        format_errors[\"data_type\"] += 1\n","        continue\n","\n","    messages = ex.get(\"messages\", None)\n","    if not messages:\n","        format_errors[\"missing_messages_list\"] += 1\n","        continue\n","\n","    for message in messages:\n","        if \"role\" not in message or \"content\" not in message:\n","            format_errors[\"message_missing_key\"] += 1\n","\n","        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n","            format_errors[\"message_unrecognized_key\"] += 1\n","\n","        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n","            format_errors[\"unrecognized_role\"] += 1\n","\n","        content = message.get(\"content\", None)\n","        if not content or not isinstance(content, str):\n","            format_errors[\"missing_content\"] += 1\n","\n","    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n","        format_errors[\"example_missing_assistant_message\"] += 1\n","\n","if format_errors:\n","    print(\"Found errors:\")\n","    for k, v in format_errors.items():\n","        print(f\"{k}: {v}\")\n","else:\n","    print(\"No errors found\")"],"metadata":{"id":"moho_aWELkL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Token counting functions\n","encoding = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# not exact!\n","# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n","def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n","    num_tokens = 0\n","    for message in messages:\n","        num_tokens += tokens_per_message\n","        for key, value in message.items():\n","            num_tokens += len(encoding.encode(value))\n","            if key == \"name\":\n","                num_tokens += tokens_per_name\n","    num_tokens += 3\n","    return num_tokens\n","\n","def num_assistant_tokens_from_messages(messages):\n","    num_tokens = 0\n","    for message in messages:\n","        if message[\"role\"] == \"assistant\":\n","            num_tokens += len(encoding.encode(message[\"content\"]))\n","    return num_tokens\n","\n","def print_distribution(values, name):\n","    print(f\"\\n#### Distribution of {name}:\")\n","    print(f\"min / max: {min(values)}, {max(values)}\")\n","    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n","    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"],"metadata":{"id":"gPC_AxVWLq1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Warnings and tokens counts\n","n_missing_system = 0\n","n_missing_user = 0\n","n_messages = []\n","convo_lens = []\n","assistant_message_lens = []\n","\n","for ex in dataset:\n","    messages = ex[\"messages\"]\n","    if not any(message[\"role\"] == \"system\" for message in messages):\n","        n_missing_system += 1\n","    if not any(message[\"role\"] == \"user\" for message in messages):\n","        n_missing_user += 1\n","    n_messages.append(len(messages))\n","    convo_lens.append(num_tokens_from_messages(messages))\n","    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n","\n","print(\"Num examples missing system message:\", n_missing_system)\n","print(\"Num examples missing user message:\", n_missing_user)\n","print_distribution(n_messages, \"num_messages_per_example\")\n","print_distribution(convo_lens, \"num_total_tokens_per_example\")\n","print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n","n_too_long = sum(l > 4096 for l in convo_lens)\n","print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"],"metadata":{"id":"VI1sbe6zLrp3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pricing and default n_epochs estimate\n","MAX_TOKENS_PER_EXAMPLE = 4096\n","\n","TARGET_EPOCHS = 3\n","MIN_TARGET_EXAMPLES = 100\n","MAX_TARGET_EXAMPLES = 25000\n","MIN_DEFAULT_EPOCHS = 1\n","MAX_DEFAULT_EPOCHS = 25\n","\n","n_epochs = TARGET_EPOCHS\n","n_train_examples = len(dataset)\n","if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n","    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n","elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n","    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n","\n","n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n","print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n","print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n","print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n","print(\"See pricing page to estimate total costs\")\n"],"metadata":{"id":"DomOc2YzL9xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def save_to_jsonl(conversations, file_path):\n","    with open(file_path, 'w') as file:\n","        for conversation in conversations:\n","            json_line = json.dumps(conversation)\n","            file.write(json_line + '\\n')"],"metadata":{"id":"6nqR5W_NWqmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify the full path including the file name and extension\n","file_path = '/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetunedDataset/finetuning_dataset.jsonl'\n","\n","# Now, call your function with the corrected path\n","save_to_jsonl(dataset, file_path)\n"],"metadata":{"id":"mvci7_E3WvgR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create Train Test Data\n"],"metadata":{"id":"gYUi-pZ1Hver"}},{"cell_type":"code","source":["pip install pandas scikit-learn"],"metadata":{"id":"KWkRohutINFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","# Mount Google Drive (follow the link and enter the authorization code)\n","drive.mount('/content/drive')"],"metadata":{"id":"NIMMVu9iIGjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_json('/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetunedDataset/finetuning_dataset.jsonl', lines=True)\n","\n","# Display the first few rows of the dataframe\n","print(df.head())\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split the data\n","train_df, validation_df = train_test_split(df, test_size=0.1)  # 80% training, 20% validation\n","\n","# You can adjust the `test_size` parameter as needed\n","train_df.to_json('/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetunedDataset/train_finetuning_dataset.jsonl', orient='records', lines=True)\n","validation_df.to_json('/content/drive/MyDrive/BulkTestGPT35AlmaanyDictionarySinglePromptFinetunedDataset/val_finetuning_dataset.jsonl', orient='records', lines=True)\n"],"metadata":{"id":"sZTlKzquH03i"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["lcxGOOLH3zOJ","J8cplOgkLkmI","47S0-VNA-8Jx","1vXA_YF0z6hv","G0eGoQ9M8nRU","pNGMiw1n8zN4","SapARhGiScrA","gYUi-pZ1Hver"],"machine_shape":"hm","provenance":[{"file_id":"1LRTr9gmR5P6u9vpXZeUXcZjebtD0vrzj","timestamp":1709330147656},{"file_id":"1k-db9guSQNDPzwxNsDRQnfjMJ-gHx3g0","timestamp":1709070333421},{"file_id":"1mcz0D6_CSxP7PI-lZJDGIhYPzbaII6bn","timestamp":1709069657735},{"file_id":"1vciZ-7uIJ-9I8XOV1jMb_i3kkqA6isXR","timestamp":1708592281472},{"file_id":"1ytLiTT5Q7TAbFuWam787_CyCBpaBQD_U","timestamp":1708592253580},{"file_id":"1b1A3eb6dLZisxvp1KiTXOAXULF_kX2Ly","timestamp":1708591444026}],"mount_file_id":"1mcz0D6_CSxP7PI-lZJDGIhYPzbaII6bn","authorship_tag":"ABX9TyNMvmgG/ACahBCf0McfYd6a"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}